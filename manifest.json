{
  "$schema": "https://stina.app/schemas/extension-manifest.json",
  "id": "ollama-provider",
  "name": "Ollama AI Provider",
  "version": "1.0.9",
  "description": "Connect Stina to your local Ollama instance for private, offline AI conversations.",
  "type": "provider",
  "author": {
    "name": "Stina Team",
    "url": "https://github.com/einord"
  },
  "repository": "https://github.com/einord/stina-ext-ollama",
  "license": "MIT",
  "engines": {
    "stina": ">=0.5.0"
  },
  "platforms": ["web", "electron", "tui"],
  "main": "index.js",

  "permissions": [
    "network:*",
    "provider.register"
  ],

  "contributes": {
    "providers": [
      {
        "id": "ollama",
        "name": "Ollama",
        "description": "Local AI models via Ollama",
        "suggestedDefaultModel": "llama3.2:8b",
        "defaultSettings": {
          "url": "http://localhost:11434",
          "thinking": "off"
        },
        "configSchema": {
          "order": ["url", "thinking"],
          "properties": {
            "url": {
              "type": "url",
              "title": "Server URL",
              "description": "URL to your Ollama server",
              "placeholder": "http://localhost:11434",
              "default": "http://localhost:11434",
              "required": true
            },
            "thinking": {
              "type": "select",
              "title": "Thinking Mode",
              "description": "Enable thinking/reasoning for supported models (Qwen3, DeepSeek R1, etc.)",
              "default": "off",
              "options": [
                { "value": "off", "label": "Off" },
                { "value": "on", "label": "On" },
                { "value": "low", "label": "Low (extended)" },
                { "value": "medium", "label": "Medium (extended)" },
                { "value": "high", "label": "High (extended)" }
              ]
            }
          }
        }
      }
    ]
  }
}
